{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from backtester import matlab, backtester\n",
    "from backtester.analysis import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os\n",
    "\n",
    "from sklearn import linear_model, cross_validation, naive_bayes, ensemble, tree, neighbors, cluster, preprocessing\n",
    "\n",
    "from pandas_datareader import data, wb\n",
    "import datetime\n",
    "\n",
    "%pylab inline\n",
    "figsize(20,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETF ticker loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tickers_df = pd.read_html('http://etfdb.com/screener/')\n",
    "tickers_df = tickers_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = datetime.datetime(2010, 1, 1)\n",
    "end = datetime.datetime.now()\n",
    "\n",
    "for s in tickers_df.Symbol[0:1000]:\n",
    "    try:\n",
    "        t = data.DataReader(s, 'yahoo', start, end)\n",
    "        t_close = pd.Series(t['Adj Close'], name=s)\n",
    "        t_close.to_hdf('./tickers/' + s + '.h5', 'adjclose')\n",
    "        \n",
    "    except Exception:\n",
    "        print('Problem with downloading - ', s)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ticker_folder = './zscore_tickers/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_data_l = []\n",
    "temp_name_l = []\n",
    "\n",
    "tickers_df = pd.DataFrame()\n",
    "\n",
    "direction_flag = 'long'\n",
    "\n",
    "for file in glob.glob(ticker_folder + \"*.h5\"):\n",
    "    \n",
    "    if direction_flag == 'long':\n",
    "        ticker = pd.read_hdf(file)\n",
    "        \n",
    "    elif direction_flag == 'short':\n",
    "        ticker = (pd.read_hdf(file)) * -1\n",
    "    \n",
    "    else:\n",
    "        raise Exception('Invalid long/short flag. Set flag to \"long\" or \"short\"')\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "    if int(ticker.index.year[0]) <= 2011: \n",
    "        tickers_df[ticker.name] = ticker\n",
    "\n",
    "        price_change = ticker - ticker.shift(1)\n",
    "        max_dd = (ticker - ticker.expanding().max()).min()\n",
    "\n",
    "        period_end_price = ticker[-1]\n",
    "        period_start_price = ticker[0]\n",
    "        \n",
    "        median_vola = (ticker - ticker.shift(1)).rolling(30).median()\n",
    "        \n",
    "        \n",
    "        dd_lengths_d = {}\n",
    "\n",
    "        for y in numpy.unique(ticker.index.year):\n",
    "\n",
    "            expmax = ticker.ix[str(y)].expanding().max()\n",
    "\n",
    "            same_max_bars = np.zeros_like(expmax.values)\n",
    "            same_max_counter = 0\n",
    "\n",
    "            for i in range(len(expmax)):\n",
    "                if expmax[i] == expmax.shift(1)[i]:\n",
    "                    same_max_counter += 1\n",
    "                    same_max_bars[i] = same_max_counter\n",
    "\n",
    "                elif expmax[i] != expmax.shift(1)[i]:\n",
    "                    same_max_counter = 0\n",
    "                    same_max_bars[i] = same_max_counter\n",
    "\n",
    "            dd_lengths_d[y] = np.mean(same_max_bars)\n",
    "            #print(pd.Series(same_max_bars, index=expmax.index).mean())#.plot(secondary_y=True)\n",
    "            \n",
    "        for period in [1,2,5]:\n",
    "            \n",
    "            consec_up_bars = np.zeros_like(ticker.values)\n",
    "            up_counter = 0\n",
    "\n",
    "            consec_dn_bars = np.zeros_like(ticker.values)\n",
    "            dn_counter = 0\n",
    "\n",
    "            for i in range(len(ticker)):\n",
    "                if ticker[i] > ticker.shift(period)[i]:\n",
    "                    up_counter += 1\n",
    "                    consec_up_bars[i] = up_counter\n",
    "\n",
    "                elif ticker[i] < ticker.shift(period)[i]:\n",
    "                    up_counter = 0\n",
    "                    consec_up_bars[i] = up_counter\n",
    "\n",
    "\n",
    "\n",
    "                if ticker[i] < ticker.shift(period)[i]:\n",
    "                    dn_counter += 1\n",
    "                    consec_dn_bars[i] = dn_counter\n",
    "\n",
    "                elif ticker[i] > ticker.shift(period)[i]:\n",
    "                    dn_counter = 0\n",
    "                    consec_dn_bars[i] = dn_counter\n",
    "\n",
    "            updn_consec_bars_df = pd.concat([pd.Series(consec_up_bars,name='consec_up_bars'), \n",
    "                                             pd.Series(consec_dn_bars,name='consec_dn_bars')], axis=1).drop(0)\n",
    "\n",
    "            #\n",
    "            # Same thing for 1-2-5 period shift\n",
    "            #\n",
    "            d['average_consec_up_bars_shiftperiod'+str(period)] = updn_consec_bars_df.consec_up_bars.mean()\n",
    "            d['average_consec_dn_bars_shiftperiod'+str(period)] = updn_consec_bars_df.consec_dn_bars.mean()\n",
    "            d['average_consec_updn_bars_ratio_shiftperiod'+str(period)] = (updn_consec_bars_df.consec_up_bars.value_counts() / updn_consec_bars_df.consec_dn_bars.value_counts()).mean()\n",
    "    \n",
    "        for period in [5,10,30]:\n",
    "            ema = ticker.ewm(period).mean()\n",
    "\n",
    "            crossup = CrossUp(ticker, ema)\n",
    "            crossdn = CrossDown(ticker, ema)\n",
    "\n",
    "            days_wo_crossings_count = np.zeros_like(ticker.values)\n",
    "            days_wo_crossings_counter = 0\n",
    "\n",
    "            for i in range(len(ticker)):\n",
    "                if (crossup[i] == False) | (crossdn[i] == False):\n",
    "                    days_wo_crossings_counter += 1\n",
    "                    days_wo_crossings_count[i] = days_wo_crossings_counter\n",
    "\n",
    "                if (crossup[i] == True) | (crossdn[i] == True):\n",
    "                    days_wo_crossings_counter = 0\n",
    "                    days_wo_crossings_count[i] = days_wo_crossings_counter\n",
    "                    \n",
    "            d['median_days_wo_crossings_ema'+str(period)] = pd.Series(days_wo_crossings_count).median()\n",
    "            d['quantile90_days_wo_crossings_ema'+str(period)] = pd.Series(days_wo_crossings_count).quantile(0.8)\n",
    "        \n",
    "        \n",
    "        d = {'pricechange_modsharpe': np.mean(price_change) / np.std(price_change), 'max_dd': max_dd, \n",
    "             'recovery_factor': period_end_price / np.abs(max_dd), \n",
    "             'pct_change1_skew': ticker.pct_change(1).skew(), 'pct_change1_mean': ticker.pct_change(1).mean(),\n",
    "             'pct_change1_std': ticker.pct_change(1).std(), 'pct_change1_var': ticker.pct_change(1).var(),\n",
    "             'pct_change5_skew': ticker.pct_change(5).skew(), 'pct_change5_mean': ticker.pct_change(5).mean(),\n",
    "             'pct_change5_std': ticker.pct_change(5).std(), 'pct_change5_var': ticker.pct_change(5).var(),\n",
    "             'price_var': ticker.var(), 'price_std': ticker.std(), 'median_vola_median': median_vola.median(),\n",
    "             'median_vola_quantile90': median_vola.quantile(0.9), 'median_vola_quantile10': median_vola.quantile(0.1),\n",
    "             'median_vola_range': median_vola.quantile(0.9) - median_vola.quantile(0.1), 'average_dd_length': np.mean(list(dd_lengths_d.values())),\n",
    "            }\n",
    "        \n",
    "        #for k in dd_lengths_d.keys():\n",
    "        #    d['average_dd_length_in_'+str(k)] = dd_lengths_d[k]\n",
    "            \n",
    "\n",
    "        temp_data_l.append(d)\n",
    "        temp_name_l.append(ticker.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ticker = pd.read_hdf('./tickers\\\\MFLA.h5')\n",
    "#ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ticker_stats_df = pd.DataFrame(temp_data_l, index=temp_name_l).dropna()\n",
    "ticker_stats_df.sort_values('average_dd_length', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tickers_df[ticker_stats_df.sort_values('average_dd_length', ascending=True).index[12]].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ticker_stats_df2 = pd.DataFrame(preprocessing.normalize(ticker_stats_df), index=ticker_stats_df.index,\n",
    "                                         columns=ticker_stats_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clustering_model = cluster.KMeans(10)\n",
    "clustering_model.fit(ticker_stats_df2.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ticker_stats_df['cluster'] = clustering_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figsize(20,10)\n",
    "\n",
    "# Change this variable to select a strategies in the cluster\n",
    "cluster_n = 2\n",
    "\n",
    "\n",
    "tickers_df[ticker_stats_df[ticker_stats_df.cluster == cluster_n].index].sum(1).plot(legend=False)\n",
    "tickers_df[ticker_stats_df[ticker_stats_df.cluster == cluster_n].index].plot(legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ticker_stats_df[ticker_stats_df.cluster == cluster_n]\n",
    "ticker_stats_df.cluster.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tickers_df[ticker_stats_df[ticker_stats_df.cluster == 1].index]#.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ticker = tickers_df.sample(1, axis=1)\n",
    "\n",
    "ticker.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simple volatility stats\n",
    "print(ticker.ix['2014'].std()) # overall stats and by year(?)\n",
    "print(ticker.ix['2014'].var()) # overall stats and by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simple ptice change stats\n",
    "# Also for periods 5-10-20-100\n",
    "print(ticker.pct_change(10).skew())\n",
    "print(ticker.pct_change(10).mean())\n",
    "print(ticker.pct_change(10).median())\n",
    "print(ticker.pct_change(10).var())\n",
    "print(ticker.pct_change(10).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(ticker - ticker.shift(1)).rolling(30).median().plot()\n",
    "(ticker - ticker.shift(1)).rolling(30).median().median()\n",
    "axhline((ticker - ticker.shift(1)).rolling(30).median().median())\n",
    "\n",
    "axhline((ticker - ticker.shift(1)).rolling(30).median().quantile(.9))\n",
    "axhline((ticker - ticker.shift(1)).rolling(30).median().quantile(.1))\n",
    "\n",
    "# Volatility range\n",
    "print(((ticker - ticker.shift(1)).rolling(30).median().quantile(.8) - (ticker - ticker.shift(1)).rolling(30).median().quantile(.2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Days after renewing a historical low or high\n",
    "# Length of drawdowns ?\n",
    "ticker.plot()\n",
    "\n",
    "ticker.ix['2011'].expanding().max().plot()\n",
    "ticker.ix['2011'].expanding().min().plot()\n",
    "\n",
    "dd_lengths_d = {}\n",
    "\n",
    "for y in numpy.unique(ticker.index.year):\n",
    "    \n",
    "    expmax = ticker.ix[str(y)].expanding().max()\n",
    "\n",
    "    same_max_bars = np.zeros_like(expmax.values)\n",
    "    same_max_counter = 0\n",
    "\n",
    "    for i in range(len(expmax)):\n",
    "        if expmax[i] == expmax.shift(1)[i]:\n",
    "            same_max_counter += 1\n",
    "            same_max_bars[i] = same_max_counter\n",
    "\n",
    "        elif expmax[i] != expmax.shift(1)[i]:\n",
    "            same_max_counter = 0\n",
    "            same_max_bars[i] = same_max_counter\n",
    "    \n",
    "    dd_lengths_d[y] = np.mean(same_max_bars)\n",
    "    print(pd.Series(same_max_bars, index=expmax.index).mean())#.plot(secondary_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(list(dd_lengths_d.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Consecutive up/down bars count, ratios\n",
    "\n",
    "consec_up_bars = np.zeros_like(ticker.values)\n",
    "up_counter = 0\n",
    "\n",
    "consec_dn_bars = np.zeros_like(ticker.values)\n",
    "dn_counter = 0\n",
    "\n",
    "for i in range(len(ticker)):\n",
    "    if ticker[i] > ticker.shift(1)[i]:\n",
    "        up_counter += 1\n",
    "        consec_up_bars[i] = up_counter\n",
    "        \n",
    "    elif ticker[i] < ticker.shift(1)[i]:\n",
    "        up_counter = 0\n",
    "        consec_up_bars[i] = up_counter\n",
    "      \n",
    "    \n",
    "    \n",
    "    if ticker[i] < ticker.shift(1)[i]:\n",
    "        dn_counter += 1\n",
    "        consec_dn_bars[i] = dn_counter\n",
    "        \n",
    "    elif ticker[i] > ticker.shift(1)[i]:\n",
    "        dn_counter = 0\n",
    "        consec_dn_bars[i] = dn_counter\n",
    "        \n",
    "updn_consec_bars_df = pd.concat([pd.Series(consec_up_bars,name='consec_up_bars'), \n",
    "                                 pd.Series(consec_dn_bars,name='consec_dn_bars')], axis=1).drop(0)\n",
    "\n",
    "#\n",
    "# Same thing for 1-2-5 period shift\n",
    "#\n",
    "\n",
    "print(updn_consec_bars_df.consec_up_bars.mean())\n",
    "\n",
    "print(updn_consec_bars_df.consec_dn_bars.mean())\n",
    "\n",
    "print((updn_consec_bars_df.consec_up_bars.value_counts() / updn_consec_bars_df.consec_dn_bars.value_counts()).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(consec_up_bars).hist(bins=30)#.plot()\n",
    "pd.Series(consec_dn_bars).hist(bins=30)#.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining a Trend/meanrev regimes by EMA crossing\n",
    "\n",
    "# Periods for EMA 5-10-30\n",
    "ema = ticker.ewm(5).mean()\n",
    "\n",
    "ticker.plot()\n",
    "CrossUp(ticker, ema).plot(secondary_y=True)\n",
    "CrossDown(ticker, ema).plot(secondary_y=True)\n",
    "\n",
    "crossup = CrossUp(ticker, ema)\n",
    "crossdn = CrossDown(ticker, ema)\n",
    "\n",
    "days_wo_crossings_count = np.zeros_like(ticker.values)\n",
    "days_wo_crossings_counter = 0\n",
    "\n",
    "for i in range(len(ticker)):\n",
    "    if (crossup[i] == False) | (crossdn[i] == False):\n",
    "        days_wo_crossings_counter += 1\n",
    "        days_wo_crossings_count[i] = days_wo_crossings_counter\n",
    "                \n",
    "    if (crossup[i] == True) | (crossdn[i] == True):\n",
    "        days_wo_crossings_counter = 0\n",
    "        days_wo_crossings_count[i] = days_wo_crossings_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ticker.plot()\n",
    "#CrossUp(ticker, ema).plot(secondary_y=True)\n",
    "#CrossDown(ticker, ema).plot(secondary_y=True)\n",
    "\n",
    "pd.Series(crossings_count, index=ticker.index).plot(secondary_y=True)\n",
    "#axhline(pd.Series(crossings_count, index=ticker.index).mean())\n",
    "\n",
    "axhline(pd.Series(crossings_count, index=ticker.index).median())\n",
    "\n",
    "#axhline(pd.Series(crossings_count, index=ticker.index).quantile(0.9))\n",
    "\n",
    "axhline(pd.Series(crossings_count, index=ticker.index).quantile(0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(crossings_count, index=ticker.index).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
